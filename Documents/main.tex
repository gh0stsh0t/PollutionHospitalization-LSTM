\documentclass[10pt,11pt,12pt,oneside]{book}
\usepackage{multicol}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{emptypage}
\usepackage{datetime}
\usepackage{indentfirst}
\usepackage{placeins}
\usepackage{subfig}
\usepackage[svgnames]{xcolor}
\usepackage[flushleft]{threeparttable}
\definecolor{grey}{RGB}{183,183,183}
\usepackage[font={color=grey},figurename=figure.]{caption}
\usepackage[left=1.2in,right=1in,top=1in,bottom=.8in]{geometry}
\usepackage{tikz}
\usepackage{booktabs}
\usepackage{float}
\iffalse
\begin{figure}
         \includegraphics[width=\linewidth]{results.png}
         \caption{Prediction Results of each Models}
         \label{fig:results}
\end{figure}
\fi


\title{Predicting Lung Related Hospitalization in Davao City due to Air Quality via Recurrent Neural Networks}
\author{rjsperias}
\date{July 2018}
\begin{document}
\cleardoublepage
\begin{titlepage}
    \begin{center}
        \includegraphics[width=1.5in]{addu.pdf}\\
        \vspace{1cm}
        \huge{Predicting Lung Related Hospitalization  in Davao City due to Air Quality via Recurrent Neural Networks}\\
        \vspace{1.5in}
        \large{Christian Owenn C. de Jesus}\\
        \large{Raphael John S. Perias}\\
        \vspace{1.5in}
        \large{ATENEO DE DAVAO UNIVERSITY\\SCHOOL OF ARTS AND SCIENCES\\DAVAO CITY}\\
        \vspace{1in}
        October 2018
    \end{center}
\end{titlepage}
\frontmatter
\chapter*{Abstract}
Air pollution is a growing concern in the developing world, and its presence is difficult to realize. Continuous exposure to pollution increases the risk of developing lung-related diseases.
However, due to the numerous factors affecting the development of disease, thorough statistical models have not been formulated that can reliably fit pollution concentration and change in hospitalizations from lung diseases. With the emergence of machine learning, more specifically Long Short-Term Memory (LSTM) based Recurrent Neural Networks (RNN), this study aims to develop a model that could reliably predict the changes in hospitalization based on the characteristics of an area and its pollution, taking into account the pollution present in previous days in a time-series. This model could then be used as a baseline for hospitals to distribute resources and overall be more efficient in administering aid to patients.
\tableofcontents{Table of Contents}
\mainmatter
\chapter{Introduction}
\section{Background of the Study}
    Increase in the concentration of air pollutants in various regions of the globe also increase the risk of death from lung complications. Due to the nature of air pollution, it is not easy to realize the presence and dangers of continued exposure. The human body can filter the air that is inhaled. However, it is not enough to remove all the risks from particles and pollutants ever present in the air.\cite{1971} Air pollutants are commonly not visible to the naked eye and could be small enough to penetrate conventional face masks. \cite{Shakya2016} Thus, without any visible indication, it becomes a factor for lung complications. \cite{2009} All lung complications have the symptoms of coughing and shortness of breath, which causes them to be dismissed as a simple cold or similar. Such is the case until the symptoms persist for several weeks and upon checkup, the person is diagnosed with severe diseases such as Pneumonia or Chronic bronchitis. Illustrating the risk from data collected by sensors could alert the person of what diseases he or she could contract from the polluted air.

    There are multiple studies that measured the pollution density in countries. There were also studies that took this pollution density and aligned it with the amount of hospitalized for lung-related diseases. These studies took note of environmental conditions and measured the trends in pollution for certain time periods. One such study was by Mansourian et al. (2011). However, data and numbers were the focus of these studies. The conclusion from observing the data was the presence of a statistically significant linear relationship between pollution density and number of hospitalized individuals. \cite{Mansourian2011}

    Other studies had organized previous data, while also including other factors such as age and previous lung complications, such as asthma. These studies had results trying to correlate risk factors and the actual risk of disease development. According to the study by Amadeo et al. (2015) children who have asthma who are continuously exposed to low concentration of pollution are found to have reduced lung function afterwards. However, a thorough statistical model has not been created due to the number of factors involved. \cite{Amadeo2015}

    More recently, from the development of machine learning techniques, algorithms have been made to relate risk factors with the percent risk of the onset of diseases. Although, these algorithms are currently applied to heart complications and post-organ transplant diseases. As the risk factors for these conditions are almost exclusively found within the body, for example, Body Fluid Analysis, Imaging, and Electric Pulse Measures. Thus, more data is compiled in a standard format, making the development of algorithms faster and easier.
    \section{Statement of the Problem}

    The study aims to develop a method to use data collected by sensors in real time to illustrate the correlation of hospitalization due to lung diseases and pollutants. Through a machine learning approach, the data will be aggregated and used to generate a model for prediction. This study also seeks to answer the following questions:
    \begin{enumerate}
        \item How to determine the concentration of pollutants in the air
        \item How to predict the number of lung-related hospitalizations due to poor air quality using RNN.
        \item How to demonstrate the accuracy of RNN model in predicting lung-related hospitalizations due to poor air quality.
    \end{enumerate}
    \section{Objectives}
    The study aims to develop a method to use data collected by sensors to illustrate the correlation of hospitalization due to lung diseases and pollutants and predict the changes based on the concentration of pollution. Additionally, it intends to accomplish the following objectives:
    \begin{enumerate}
        \item To explain the methods of determining the concentration of pollutants in the air
        \item To explain how to predict the number of lung-related hospitalizations due to poor air quality using RNN.
        \item To explain how to demonstrate the accuracy of RNN model in predicting lung-related hospitalizations due to poor air quality.
    \end{enumerate}
    \section{Significance of the Study}
    The results of the study could be used in health and air monitoring systems. The study could also be used for estimating the needed supplies by a hospital, should the change in hospitalization dramatically increase, institutions could be prepared, and treatments be administered much more efficiently.

    Additionally, it could be a starting point for future studies relating the multiple variables involved in lung health of individuals and or a population in an area. Should the focus be the algorithm, accuracy and speed could be improved, thus making a better implementation. On the other hand, it could give an insight on how each factor interacts with the human body and further studies on the effect of specific matter to the human body.
    \section{Scope and Limitations}
    The study utilizes local data sourced from institutions in Davao city which contain the relevant data and ignoring those which lack several factors. The study only estimated the total number of hospitalization from any lung disease and does not assume which specific disease it is, as it requires very specific data and situations. The study utilized an NVIDIA GTX 1060 6GB GPU for training the neural network from the lack of funds to apply for a cloud computing service.\\

    Hospitalization data was requested, and given by the following institutions, Department of Health, City Health Office Davao, Davao Medical School Foundation, and Davao Doctors Hospital. Unfortunately only the data received from Davao Doctors was usable, as other data were either very sparse or were recorded in different time intervals. In addition, the data acquired from Davao Doctors Hospital was recorded monthly, although daily and weekly data could be retrieved from
their system it would take a longer time. Initially, the data intervals of hospitalizations and pollution concentration should have been daily for maximum efficiency of the model, but due to lack of data from other approached hospitals and departments, the interval was changed to monthly.
\chapter{Review of Related Literature}
    \section{Air Pollutants and correlation to Health Exposure}
        \subsection{Particulate Matter}
        Particles are small objects that can be ascribed with properties such as volume or mass. It includes hazardous stuff such as dirt and soot, being one of the oldest air pollutants. They enter the atmosphere through natural or anthropogenic means \cite{knudsen2012particulate}. When particles become suspended in the air or atmosphere for long periods of time, they are now known as Particulate Matter, one of the common hazardous form of air pollutants.

        They can be classified as primary particles, which are particles that are directly emitted into the atmosphere or form by rapid condensation of gas molecules, or secondary particles which are formed from chemical transformation of existing gases (e.g. sulfur dioxide, nitrogen oxides, ammonia, etc.). \cite{finlayson2000chemistry}

        They are generally found in heavily urbanized areas, areas such as mines or factories that uses several chemicals, and even in homes. Studies stated by Rasmussen et al. in his work in 2012 show that 4 to 8\% percent of the total annual mortality rate are caused by air pollution due to PM alone in homes. PM2.5 and PM10 (2.5 and 10 denoting size in micrometers) are associated with cardiovascular and respiratory effects, respectively. Long-term exposure to PM2.5 have confirmed links with increasing risks of pneumonia \cite{Neupane2010} and exposure to PM10 has indeed have relations with increase mortality due to pneumonia and Chronic obstructive pulmonary disease (COPD), with mortality rates being modified depending on the overall characteristics of the city \cite{MedinaRamn2006}
        \subsection{Atmosphere-based Chemical Substances}
        Chemical-based substances also play a big role in air pollution and health risks. Besides the fact that some particulate matter, as mentioned, are formed from chemical gases, the latter themselves are already poisonous to humans. When it comes to the air, harmful substances such as nitrogen oxides, ozone, and sulfur dioxide come into play.

        In studies stated by Chauhan \& Johnston in their book in 2003, there is a 1.3\% increase in daily mortality rate with the increase of NO2 or O3 at 50 micrograms/m3. In addition, increased exposure of SO2 on asthmatic children led to increased respiratory symptoms, fever, and asthmatic medication. Furthermore, fine particles associated with the SO2 increased dyspnoea in children with mild asthma. \cite{doi:10.1093/bmb/ldg022}
        \subsection{Air Quality Index/Directive}
        The Air Quality Index/Directive (AQI/AQD) is a number used to summarize or describe the current pollution levels of the air or atmosphere. Each country has its own AQI, and each levels in the AQI denote different levels of air pollution and adverse health risks as stated by the Environmental Protection Agency (EPA) in their website. \cite{airbasics}
        \subsection{Low-cost Sensors}
        According to Nuria et al.
 (2016), current monitoring systems monitor air pollution concentration through measuring pollutants such as carbon monoxide, nitrogen oxide, and particulate matter (PM10 and PM2.5). Most of these sensors are very expensive in installing and maintaining; hence, they tested several commercial low-cost sensors (AQMesh v3.5) in their study.

        When measuring the concentration of several pollutants, the expanded uncertainty met the DQO criteria in the Air Quality Directive for some pollutants like NO, PM10, and PM2.5 while other pollutants like CO and NO2 showed expanded uncertainty exceeding the Data Quality Objectives’ (DQO) indicative methods. In short, the commercial low-cost sensors were deemed unsuitable; however, recent studies have shown that the application of field calibrations based on machine learning techniques can reduce the expanded uncertainty and improve overall performance. \cite{Castell2017}
    \section{Types of Recurring Neural Networks}
        \subsection{Long Short-Term Memory (LSTM)}
        Hochreiter and Schmidhuber [1997] invented Long Short-Term Memory networks, which are designed to solve the vanishing, and exploding gradient problem due to back-propagation when training artificial neural networks. Since then, it has been used in applications such as image processing and classification due to their ridiculous accuracy, and for their ability to store information over long-time intervals with minimal to no risk of it getting lost. \cite{Hochreiter:1997:LSM:1246443.1246450}

        A study by Zachary, et al. [2016] tested the efficiency of LSTM in diagnosing time series data gathered in the intensive care unit (ICU). While the networks were able to produce impressive results, there are some areas where LSTM performed worse due to the fact that some data progression does not follow with the standard linear progression commonly used in networks. \cite{DBLP:journals/corr/LiptonKEW15}
        \subsection{Gated Recurrent Unit (GRU)}
        GRUs are RNNs that functions mostly like LSTMs albeit with key differences, mainly that GRUs has fewer gates and that it is more efficient in sequence modeling and time series data analyzing when there are fewer parameters, and gathered data are used in short-time intervals (Britz, 2015). \cite{britz_2016}

        Zhengping et al. (2016) experimented on GRUs using multivariate series with missing values. The proponents wanted to exploit those missing values as a means to improve overall predicting performance in models; hence, the proponents created a new model called GRU-D, GRUs with decay mechanism that addresses the overall impact of missing values in the data in specific time intervals. The model itself proved valuable; it effectively handled the missing values in multivariate data through incorporating masking and time interval directly inside GRU architecture. Moreover, the model was able to make accurate early predictions before seeing all of the time series data, making it useful in fields such as healthcare. \cite{DBLP:journals/corr/ChePCSL16}
    \section{Other Machine Learning Approaches in Other Fields}
    Machine learning has been a definite staple in computer science, especially with the exploration and construction of algorithms. With risk estimation and prediction, machine learning has been proven to be useful in risk estimation, with its different methods being underutilized and can be further studied and can be further studied and polished. \cite{Kruppa2012} Other machine-learning approaches besides neural networks have also been used in other fields of risk estimation and prediction.
        \subsection{Artificial Neural Network}
        One study by Weng et al. (2017) tested four machine-learning approaches, neural networks included, against existing algorithms of cardiovascular risk prediction. The study proved that these machine-learning approaches significantly improved the overall performance of detecting individuals who will develop CVD and those that will not, with neural networks performing the best with predictive accuracy improved by 3.6\%. \cite{Weng2017}

        \subsection{Feedforward Neural Network}
        Studies by Jain et al. (2013) and Nasri (2010) showed that Artificial Neural Networks are not only good at predicting health risks but also managing risks in general. The ANNs used are of Feedforward (FFNN) type, which are neural networks that consist of nodes that don’t form a cycle; information goes in only one way (Hornik et al., 1988). Jain et al.’s model has an accuracy up to 90\% with a very low false positive rate (FPR) of 8.3\% at max. Nasri’s model has a much higher efficiency rate in risk management compared to fuzzy logic system and genetic algorithms. \cite{jain_singh_2013, nasri2010application}

        \subsection{Random Forest}
        A study by Gurm et al. (2014) created and analyzed a random forest-based risk model of receipt of transfusion in patients undergoing Percutaneous Coronary Intervention. The model, restricted based on Michigan, was indeed successful, albeit improvements can still be made. The model helped identify patient subgroups with a higher or lower risk of needing a transfusion, and it also proved PCI can be reliably estimated. \cite{Gurm2014}

        Another study by Ghatasheh (2014) used random forest trees to evaluate credit risk. Due to other approaches being black-boxed, random forest was suggested since it gives some insight on the dynamics and interactions between the underlying factors of credit risks. The results of the research show that random forest trees are a promising opportunity in Business Analytics, with pros like effective classification and accuracy being more beneficial to business domain experts. The findings also show how to improve decision trees in general through thorough investigation, testing and improving classification models. \cite{Ghatasheh2014BusinessAU}
        \subsection{Logistic Regression}
        Yanfeng et al. (2017) used logistic regression to create a risk model for pneumothorax after CT-guided needle biopsy and the different risk factors involved. Using univariate logistic regression analysis to determine risk factors, and then multivariate logistic regression analysis to break down which aforementioned risk factors are independent of each other, a regression analysis formula was formed to create the model. The model, integrated with all independent risk factors, predicted pneumothorax among patients with high accuracy. \cite{Zhao2017}
        \subsection{Gradient Boosting}
        Atkinson et al. (2013) used gradient boosting machine models (GBM) to assess whether it can improve bone fracture prediction. Gradient boosting “focuses on combining information from many variables that individually may not be significant but together are very informative.” Taking advantage of all the variables needed for the analysis, the GBM were able to differentiate fracture and non-fracture subjects with surprisingly high predictive ability.

        The study demonstrated that GBM has the potential to be very effective in its risk predicting in terms of the medical fields, precisely because of 3 reasons as stated by Atkinson et al.: (1) there is evidence that boosting methods are one of the approaches least affected by overfitting; (2) GBM models’ non-linearity and interactions between variables can be captured without prior specification; and (3) GBM incorporates the stochastic component, e.g. falling, that is so important in fracture pathogenesis. \cite{Atkinson2012}
    \section{Root-Mean-Square Error (RMSE)}
    The Root-Mean-Square Error is a statistical metric commonly used in measuring model performance. It is widely considered as the standard metric in model errors. As concluded by Chai et al. (2014) in their research, RMSE best represents model performance when error distribution is expected to be normal, and that it satisfies the triangle inequality requirement to be considered as a distance metric.
    
The formula, with respect to the estimated variable Xmodel is defined as the square root of the mean squared error (MSE) where Xobs is observed values and Xmodel is modeled values at time/place i.\\
    \begingroup
\Large
    \begin{math}
\centering
RMSE = \sqrt{}\frac{\sum_{n}^{i=1}\left ( X_{obs,i} - X_{model,i}\right )^{2}}{n}
\end{math}\\
\endgroup
    \section{Theoretical framework}
    \begin{figure} [ht]
        \includegraphics[width=6.5in]{rnnframework.png}
        \caption{RNN framework}
        \medskip
    \end{figure}
    A framework is created in the study conducted by Esteban, et al. wherein static and dynamic data of patients were used as predictors of future clinical events for several RNN implementations. It shows that the static information was first fed into an independent Feedforward Neural Network and the dynamic information into the RNN. Afterward, they then concatenated the hidden layers of both networks and provided the information into the output layer.

    As shown in the figure, the networks are fed latent representations of the data. The latent representations are computed by applying linear transformation on the raw input, which contains dynamic data and static data of the patient, except for the input for the independent network which only contains the static data of the patient. From the output of the network, a cost is calculated, the cost function applied was based on the Binary Cross Entropy function. \cite{DBLP:journals/corr/EstebanSYT16}
    \FloatBarrier
\chapter{Project Design and Methodology}
\section{Conceptual Framework}
\includegraphics[height=6in]{conframework.png}
\section{Methodology}
    \subsection{Gathering of Pollution and Hospitalization Data}
    Two sets of data were then collected. The First set of data, labeled as Target, contains hospitalizations sourced from medical records of Davao Doctors Hospital. The hospitalizations listed patients who were diagnosed with lung diseases; specifically, Asthma, Chronic Obstructive Pulmonary Disease, Chronic Bronchitis, Emphysema, Lung Cancer, Pneumonia, and Tuberculosis. The second set of data, labeled as Features, contains pollution data sourced from the Department of Environment and Natural
    Resources(DENR). Pollution data was given by the DENR in several pdf files, each containing daily and weekly data for a year. The data contained the measurements collected by six monitoring stations, namely: DC Station 02, DC Station 07, DC Station 11, DC Station 14, DC Station 15, and DC Station 16, which were located at Amparo Subdivision Open Lot, Barangay Compound, Brgy 12-B, Davao Memorial Park, Phase II, Toril Open Park Area, Toril Poblacion, Davao City, Davao International Airport, AGL Open
Area, and Calinan National High School, Oval Open Area, respectively. The former four stations sample the air weekly while the latter two sample continuously daily. The pollution data used for training the models consist of the levels of concentrations of different pollutants that have been detected in different areas in Davao City; namely, PM10, NO2, O3, and SO2. All data gathered took place in 2015-2017, with a monthly interval. The data represented in a graph is shown below.\\
\begin{figure}[H]
\centering
\resizebox{!}{10cm}{\input{Data/pm10.pgf}}
\caption{PM10 Readings across stations}
\label{fig:pm10graph}
\end{figure}
\begin{figure}[H]
\centering
\resizebox{!}{10cm}{\input{Data/so2.pgf}}
\caption{SO2 Readings across stations}
\label{fig:so2graph}
\end{figure}
\begin{figure}[H]
\centering
\resizebox{!}{10cm}{\input{Data/no2.pgf}}
\caption{NO2 Readings across stations}
\label{fig:no2graph}
\end{figure}
\begin{figure}[H]
\centering
\resizebox{!}{10cm}{\input{Data/o3.pgf}}
\caption{O3 Readings across stations}
\label{fig:o3graph}
\end{figure}
\pagebreak
    \subsection{Organizing Data}
Data from the PDF files given by DENR was extracted into CSV files named by the station and year the data came from e.g. "02\_15.csv". The columns were date, pm10, so2, no2, and o3, the date is formatted as week/month/year and blank entries were encoded as "nan" as shown in \ref{table:data_csv1} with statistics in \ref{table:minMax}

Data from Davao Doctors was a printed worksheet with the number of hospital cases for specific lung diseases for every month, all the cases per month were then summed and encoded into the file "MonthlyTarget.csv" with the columns date and target. the date was formatted as month/year. Sample data is shown below in \ref{table:data_csv2}\\
\begin{table}[H]
    \centering
    \begin{tabular}{lllll|llllll}
\toprule 
      &\multicolumn{4}{c}{Station 2} & \multicolumn{4}{c}{Station 7} \\
    \midrule
     & PM10 & SO2 & NO2 & O3 & PM10 & SO2 & NO2 & O3 \\
    \midrule
    Max & 69.0 & 6.1 & 29.8 & 42.09 & 84.9 & 4.7 & 35.3 & 40.56 \\
    Min & 17.8 & 0.001 & 0.15 & 0.0 & 3.03 & 0.001 & 0.43 & 0.0  \\
    Mean & 36.572 & 0.619 & 9.383 & 5.049 & 46.926 & 0.553 & 14.611 & 5.122 \\
    \midrule
    & \multicolumn{4}{c}{Station 11} & \multicolumn{4}{c}{Station 14} \\
    \midrule
    Max & 97.4 & 6.54 & 24.78 & 28.0 & 125.8 & 16.2 & 22.4 & 15.3 \\
    Min & 4.37 & 0.001 & 0.41 & 0.0 & 12.5 & 0.001 & 0.1 & 0.0 \\
    Mean & 37.370 & 0.826 & 8.565 & 3.455 & 60.422 & 0.793 & 12.106 & 2.029 \\
    \bottomrule
\end{tabular}
    \caption{Statistics across stations}
    \label{table:minMax}
\end{table}
    


\begin{table}[H]
\parbox{.45\linewidth}{
\centering
\begin{tabular}{|l|l|l|l|l|}
\toprule
date    & pm10  & so2   & no2   & o3    \\
\hline
2/9/15  & 55.30 & 2.4   & 9     & 0.4   \\
\hline
3/9/15  & 41.50 & 0.001 & 7.9   & nan   \\
\hline
4/9/15  & 96.40 & 1.10  & 11.30 & 1.10  \\
\hline
1/10/15 & 55.60 & 0.001 & 14.6  & 28    \\
\hline
2/10/15 & 35.80 & 1     & 7.2   & 0.4   \\
\hline
3/10/15 & 46.40 & 0.001 & 15    & nan   \\
\hline
4/10/15 & 81.50 & 0.001 & 6.8   & 2.9   \\
\hline
5/10/15 & 30.20 & 0.001 & 9     & nan   \\
\hline
1/11/15 & 48.90 & 6.3   & 4.4   & 0.2   \\
\hline
2/11/15 & 27.20 & 2.7   & 9.8   & 0.6   \\
\hline
3/11/15 & 21.20 & 2.5   & 6.8   & 0.2   \\
\hline
4/11/15 & 24.40 & 5.9   & 5.8   & 1.1   \\
\hline
1/12/15 & 20.40 & nan   & 5.3   & nan   \\
\hline
2/12/15 & 26.60 & 0.2   & 5.5   & 5     \\
\hline
3/12/15 & 40.40 & 0.6   & 5.9   & 3.2   \\
\hline
4/12/15 & 27.20 & 0.20  & 7.80  & 1.10  \\
\bottomrule
\end{tabular}
\caption{Sample pollution data from station 12}
\label{table:data_csv1}
}
\hfill
\parbox{.45\linewidth}{
\centering
\begin{tabular}{|l|l|}
\toprule
date  & target  \\
\hline
8/15  & 422     \\
\hline
9/15  & 460     \\
\hline
10/15 & 430     \\
\hline
11/15 & 365     \\
\hline
12/15 & 391     \\
\hline
1/16  & 503     \\
\hline
2/16  & 366     \\
\hline
3/16  & 318     \\
\hline
4/16  & 283     \\
\hline
5/16  & 286     \\
\hline
6/16  & 330     \\
\hline
7/16  & 445     \\
\hline
8/16  & 509     \\
\hline
9/16  & 460     \\
\hline
10/16 & 472     \\
\hline
11/16 & 465     \\
\hline

\end{tabular}
\caption{Sample hospitalization data}
\label{table:data_csv2}
}
\end{table}
    \subsection{Preprocessing of Data}
    The organized data was then preprocessed to fix gaps. The steps were as follows:
    \begin{enumerate}
        \item The CSV files with weekly intervals were then parsed into pandas DataFrames and all station data per year was merged based on their date, then all years were concatenated into a single DataFrame.
        \item Dropped all rows with more than 6 NaN values
        \item Replaced remaining NaN values with -1
        \item The target data was then parsed into a DataFrame, and each month's value was then divided by the number of feature rows in the corresponding month and appended
to a list the same number of times, effectively distributing the value of the target to all the weeks of the months.
        \item The date column of the features were then dropped and then rescaled to a range of [0, 1], and the targets were similarly scaled. Both target and feature data was then split 67\% train and 33\%  test. 
    \end{enumerate}
    The final number of samples after preprocesssing was 103, also take note that the average value of the target spread across the months is 101.65.
    \pagebreak
    \subsection{Defining, Training and Fitting the Model}
    Utilizing the Keras neural network library \cite{chollet2015keras} using the Tensorflow library \cite{tensorflow2015-whitepaper} as backend, the number of layers, neurons and number of features that the basic LSTM model would have were then defined. The basic LSTM model consisted of two layers with 50 and 100 neurons in each layer, 0.2 dropout, a batch size of 50 and 500 epochs with the architecture shown in figure \ref{fig:basic_arch}. The layers used were cuDNN\cite{Chetlur2014cuDNNEP} optimized versions of the LSTM named cuDNNLSTM, which takes advantage of the parallel computing power of graphics processing units (GPU). Multiple iterations of training and tuning were performed to realize the ideal number of epochs and batch sizes that would be fitted to the model.
\begin{figure}[H]
\includegraphics[width=\linewidth]{basic_architecture.png}
\caption{Basic Model Architecture}
\label{fig:basic_arch}
\end{figure}
    \subsection{Evaluating the Model}
    The trained model is run on the test set which would then collect all predictions and calculate an error score. The output shall be the calculated Root Mean Squared Error (RMSE) that gives the error in the same units as the variable itself. The data will be presented in a graph format to illustrate the improvement of the model over time.
\chapter{Results and Discussions}
\section{Experiments}

\subsection{Defining, Training and Evaluating the Model}
The following graphs and results reflect a single run unless stated otherwise.
\subsubsection{Basic Model}
In order to know the number of epochs all the models would be trained at, the basic Model with 2 layers containing 50 and 100 neurons and a Dropout of 0.2 was trained with the Early Stopping function enabled to determine when the model would no longer improve. Then it was determined that after 500 epochs the model no longer improves, and thus this would be the default number of epochs for all proceeding models.

Based on the previous research by Ruder(2016), Adam
and RMSprop are rather similar algorithms that do well in the same circumstances, however Adam slightly outperforms RMSprop, thus being the recommended optimizer. \cite{DBLP:journals/corr/Ruder16} Nonetheless both optimizers were tested using the basic model.\\


\begin{figure}[H]
    \centering
    \subfloat[train/test loss graph]{{\includegraphics[width=7.5cm,height=5cm]{Data/optimizer.pdf}}}%
    \qquad
    \subfloat[prediction graph]{{\includegraphics[width=7.5cm,height=5cm]{Data/predictions_optimizer.pdf} }}%
    \caption{Adam vs RMSprop}%
    \label{fig:basic}%
\end{figure}
\pagebreak
\begin{multicols}{2}
\centering
\begin{verbatim}
Model: basic_rmsprop2
Test RMSE: 18.68
Test  MAE: 12.33
\end{verbatim}
\begin{verbatim}
Model: basic_adam2
Test RMSE: 16.74
Test  MAE: 10.98
\end{verbatim}
\end{multicols}
From figure \ref{fig:basic} the Adam model had a lower train loss compared to the RMSprop model, but was apparently worse in the test loss. However as the RMSE was calculated in this instance, Adam showed to be a better optimizer and thus is chosen as the default for all proceeding models.\\
\subsubsection{Batch size}
The batch size was then adjusted to see whether a lower or higher batch size will improve the model. the batch size was then decreased to 20 and the RMSE decreased slightly, then it was set to 5 and the batch size had a more noticable change.\\
\begin{figure}[H]
    \centering
    \subfloat[train/test loss graph]{{\includegraphics[width=7.5cm,height=5cm]{Data/batch_size.pdf}}}%
    \qquad
    \subfloat[prediction graph]{{\includegraphics[width=7.5cm,height=5cm]{Data/predictions_batch_size.pdf} }}%
    \caption{Batch Size}%
    \label{fig:batch}%
\end{figure}
\begin{multicols}{3}
\centering
\begin{verbatim}
Model: basic_rmsprop2
Test RMSE: 18.68
Test  MAE: 12.33
\end{verbatim}
\begin{verbatim}
Model: Lower_batch2
Test RMSE: 13.43
Test  MAE: 10.66
\end{verbatim}
\begin{verbatim}
Model: Lowest_batch2
Test RMSE: 10.49
Test  MAE: 8.32
\end{verbatim}
\end{multicols}
\pagebreak
\subsubsection{Layers stacked}
Then more layers were added and the number of neurons was increased. Simply adding a layer created a difference in the train and test loss, the same was expected by increasing the neurons, however the train loss increased after several epochs and is an indicator of overfitting.\\
\begin{figure}[H]
    \centering
    \subfloat[train/test loss graph]{{\includegraphics[width=7.5cm,height=5cm]{Data/layer_3.pdf}}}%
    \qquad
    \subfloat[prediction graph]{{\includegraphics[width=7.5cm,height=5cm]{Data/predictions_layer_3.pdf} }}%
    \caption{Layers and Neurons tuned}%
    \label{fig:layers}%
\end{figure}
\begin{multicols}{3}
\centering
\begin{verbatim}
Model:Lowest_batch2
Test RMSE: 10.49
Test  MAE: 8.32
\end{verbatim}
\begin{verbatim}
Model: lower_batch3
Test RMSE: 10.65
Test  MAE: 8.38
\end{verbatim}
\begin{verbatim}
Model: high_neuron3
Test RMSE: 11.43
Test  MAE: 8.94
\end{verbatim}
\end{multicols}
\subsubsection{Grid Search}
A parameter grid based on all the models with a good RMSE score was then created and then a Grid Search was performed. The grid search iterates through all possible combinations of the parameter grid to find the best model with its parameters, The best model that was found by the grid search, which had 3 layers with 250, 300, 200 neurons with a batch size of 5, was then tested and its score is one of the best performing amongst other LSTM models\\
\begin{figure}[H]
    \centering
    \subfloat[train/test loss graph]{{\includegraphics[width=7.5cm,height=5cm]{Data/grid_search.pdf}}}%
    \qquad
    \subfloat[prediction graph]{{\includegraphics[width=7.5cm,height=5cm]{Data/predictions_grid_search.pdf} }}%
    \caption{Grid Search}%
    \label{fig:grid}%
\end{figure}
\begin{multicols}{2}
\centering
\begin{verbatim}
Model: lower_batch3
Test RMSE: 10.65
Test  MAE: 8.38
\end{verbatim}
\begin{verbatim}
Model: Grid Searched
Test RMSE: 8.56
Test  MAE: 7.04
\end{verbatim}
\end{multicols}
\subsubsection{Feed Forward Deep Neural Network}
Although outside the scope of the study, a basic Feed Forward Deep Neural Network (FFDNN) also known as a Multilayer Perceptron (MLP) was created to show the effect of a simpler model on a small dataset. This model averaged a 5.75 RMSE and is consistently more accurate than LSTM models. \\
\begin{figure}[H]
    \centering
    \subfloat[train/test loss graph]{{\includegraphics[width=7.5cm,height=5cm]{Data/ffdnn.pdf}}}%
    \qquad
    \subfloat[prediction graph]{{\includegraphics[width=7.5cm,height=5cm]{Data/predictions_ffdnn.pdf}}}%
    \caption{Multilayer Perceptron}%
    \label{fig:mlp}%
\end{figure}
\begin{multicols}{2}
\centering
\begin{verbatim}
Model: Grid Searched
Test RMSE: 8.56
Test  MAE: 7.04
\end{verbatim}
\begin{verbatim}
Model: FFDNN
Test RMSE: 5.88
Test  MAE: 3.92
\end{verbatim}
\end{multicols}
\section{Results}
Out of 22 models with different parameters which were trained and tested, 7 essential models were handpicked, and these models were trained and tested 100 times. The accuracy of the 7 models along with the grid searched model was gauged through an accompanying feed forward neural network on each run. All previous graph's data in a single run were collated into as single graph in figure \ref{fig:all_graphs}. While the RMSE, MAE, and range values of the models, grid search, and FFDNN were recorded and averaged for 100 runs. The parameters of the models are shown in table \ref{table:params} while the multiple values of the models' RMSE and MAE within 100 runs are shown in table \ref{table:results}\\
\begin{figure}[H]
    \centering
    \subfloat[train/test loss graph]{{\includegraphics[width=7.5cm,height=5cm]{Data/all_models.pdf}}}%
    \qquad
    \subfloat[prediction graph]{{\includegraphics[width=7.5cm,height=5cm]{Data/predictions_all_models.pdf} }}%
    \caption{Multilayer Perceptron}%
    \label{fig:all_graphs}%
\end{figure}
\begin{table}[H]
\centering
\begin{threeparttable}
\begin{tabular}{ |p{2cm}||p{2.5cm}|p{1.5cm}|p{1.5cm}| p{2.5cm} | p{1.5cm}| p{1.5cm} |   }
 \hline
 \multicolumn{7}{|c|}{Parameters} \\
 \hline
Models & Number of LSTM Layers & Dropout & Dense & Optimization & Epochs & Batch Size\\
 \hline
 Basic2 & 2 (50, 100) & 0.2 & 1 (1) & rmsprop & 500 & 50\\
 \hline
 Basic ADAM2 & 2 (50, 100) & 0.2 & 1 (1) & adam & 500 & 50\\
 \hline
 Lower Batch3 & 3 (50, 100, 100) & 0.2 & 1 (1) & adam & 500 & 20\\
 \hline
 Lowest Batch3 & 3 (50, 100, 100) & 0.2 & 1 (1) & adam & 500 & 5\\
 \hline
 Basic3 & 3 (150, 250, 200) & 0.2 & 1 (1) & adam & 500 & 10\\
 \hline
Higher Neurons3 & 3 (150, 300, 200) & 0.2 & 1 (1) & adam & 500 & 5\\
 \hline
 Basic4 & 4 (150, 300, 200, 450) & 0.2 & 1 (1) & adam & 500 & 20\\
 \hline
 Lower Batch4 & 4 (150, 300, 200, 450) & 0.2 & 1 (1) & adam & 500 & 5\\
 \hline
 Grid Search3 & 3 (250, 200, 300) & 0.2 & 1 (1) & adam & 500 & 5\\
  \hline
 FFDNN & N/A & N/A & 3 (12, 8, 1) & adam & 500 & 2\\
 \hline
\end{tabular}
\begin{tablenotes}
      \small
      \item parenthesis indicates number of neurons of each layer respectively
    \end{tablenotes}
\end{threeparttable}
\caption{Parameters}
\label{table:params}
\end{table}
\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|l|l|}
\toprule
Model & Ave RMSE & Range & Ave MAE & Range \\
\hline
basic\_rmsprop2  & 18.99 & 17.11-20.49 & 12.52 & 11.7-13.48 \\
\hline
basic\_adam2  & 16.71 & 15.49-18.23 & 11.41 & 10.63-12.20 \\
\hline
lower\_batch2 & 13.33 & 10.93-15.42 & 1032 & 8.48-11.69  \\
\hline
lowest\_batch2 & 11.06 & 9.21-14.85 & 8.88 & 7.51-11.57 \\
\hline
basic3 & 18.06 & 15.56-21.06 & 12.58 & 11.55-14.20 \\
\hline
lower\_batch3 & 11.15 & 9.21-15.32 & 8.57 & 7.12-11.30 \\
\hline
high\_neuron3 & 10.92 & 8.00-14.86 & 8.60 & 6.63-11.25 \\
\hline
basic4 & 13.04 & 10.04-19.31 & 10.14 & 8.27-16.54 \\
\hline
lower\_batch4 & 10.17 & 8.27-16.54 & 7.97 & 6.39-11.08 \\
\hline
Grid Searched & 10.25 & 8.08-16.23 & 7.83 & 6.17-10.76 \\
\hline
FFDNN & 5.75 & 2.55-13.34 & 4.43 & 1.85-9.76 \\
\bottomrule
\end{tabular}
\caption{Tabulated error scores}
\label{table:results}
\end{table}

\section{Discussion}
    Looking at table \ref{table:results} some initial results like the lowest\_batch2 against high\_neuron3 had a different outcome after multiple runs, as it seemed that lowest\_batch2 was better but it actually was a result of high\_neuron3 overfitting in that specific instance. With high\_neuron3 being better it seems that a higher number of layers coupled with an increase in neurons actually has a noticeable effect on the RMSE. The grid searched model also presents this effect with all its layers consisting of 200 and more neurons. Then the effect of batch size is seen in all basic to lower batch models, a large decrease in batch size also has a large decrease in the RMSE, this is a side effect of the high number of iterations of fitting, before finishing a single epoch. A batch size of 50 would have 2 steps before finishing an epoch, while a batch size of 5 would have 14 steps, thus increasing the learning capabilities of the model. Then with these effects in mind, an experiment with 4 layers LSTM was conducted, and it produced the best model amongst LSTM models. With the caveat that it has the longest training time: \begin{verbatim}
Time for high_neuron3:0:00:32.737798
Time for lower_batch4:0:01:26.715916
Time for Grid Searched:0:01:05.732816
    \end{verbatim}
    This ~50\% to ~300\% increase in training time makes it impractical to grid search, taking days to weeks to find the best model.
    In the end the two best performing models are Lower Batch4 and Grid Search3. Giving the impression that a lower batch size coupled with high neurons and number of layers would result in a better score. Based on the results, it shows that RNNs can accurately predict hospitalization rate, as they are on average 10 cases off the true value, with the true value being on average 103.
\chapter{Conclusions and Recommendations}
\section{Conclusions}
Recurrent Neural Networks work by analyzing current and previous inputs in calculating their prediction. Unlike neural networks such as Feed Forward wherein information runs in only one direction, information in RNNs runs in a loop; for every succeeding input, the information of each input is stored and taken into account in the calculation of the networks' prediction. In short, the RNNs remember previous information in their calculations. Although normal RNNs suffer from the vanishing gradient problem, wherein it struggles to remember inputs far away in the sequence, modified RNNs such as Long Short-Term Memory and Gated Recurrent Unit are used to deal with this problem, with the former being used for this study. It can be inferred that the 10\% error achieved is due to the structure of the LSTMs enabling it to remember long-term inputs despite a large number of data and epochs. 

Pollutants and their concentration levels can be measured by dedicated air pollution monitoring analyzers and sensors. In gathering the pollution data, the DENR used state-of-the-art air monitoring analyzers; namely, Open-Path Dedicated Outdoor Air Systems (DOAS) and Continuous Ambient Monitoring Stations (CAMS). These air monitoring analyzers measure pollutants such as carbon dioxide (CO2) and ozone (O3) which are used in this study. Air Quality Indices are also useful to gauge the concentration levels of pollutants, which were used by DENR to know if said levels of pollutants were dangerous or not.

Predicting the number of hospitalization cases is fairly easy outside of being accurate. The pollution data is correlated with the hospitalization cases data and is subsequently used as the dataset for the RNN models. The RNN models' parameters are tuned with the desired values of the proponents and are run for training and testing. These models then output their predicted number of hospitalization cases. Inside of being accurate, however, was the main frustration of this research. While grid searching was used to get the best parameters out of a specified range of values, it took a long time to train and compare multiple models. In the end, it all boiled down to trial and error, tuning each parameter of the model until it scores a lower error value resulting in a close to the real value of hospitalizations cases found in the gathered data. 

Determining the accuracy of the models' prediction of hospitalization cases can be measured using Root-Mean-Square Error (RMSE). The RMSE outputs the percentage number on how much the predicted numbers of the models deviated from the real number of hospitalization cases found in gathered data. Since the RMSE is expressed in the same units as the output value, it can be compared to the average value of the target. A 10.25 score compared to an average value of 103 can be likened to a $\approx$10\% deviation or $\approx$90\% accuracy. Although a model outside the scope of the study, the FFDNN, had better accuracy than the LSTM, this could be a good subject of study.


Results of the accuracy of the most efficient models Lower Batch4 and Grid Search3 indicates a correlation between air quality and hospitalization cases. For further insight, each pollutant level from a different station, or group of stations yielded the following correlation levels with Target.

\begin{center}
\begin{tabular}{|l|l|l|l|l|}
\toprule
Station    & pm10  & so2   & no2   & o3    \\
\hline
1st  & -0.00117809847548528 & 0.0649830137732408   & -0.157945979415051     & -0.175355629894131   \\
\hline
2nd  & -0.183510358538085 & 0.0478200093836347 & -0.266901411223444   & -0.180209032157528   \\
\hline
3rd  & -0.0144039541529944 & 0.0725283294960922  & -0.00177406426539516 & -0.0434073697672339  \\
\hline
4th & -0.158424266002198 & 0.0252018282149746 & -0.171389052291548  & -0.0766180987948896    \\
\bottomrule
\end{tabular}
\end{center}

Among all pollutants, Sulfur Dioxide seems to be the highest possible cause of lung-related hospitalizations cases due to having positive correlation for all stations, while the other pollutants, having negative correlation, may not necessarily lead to high number of hospitalizations cases even having high levels. At the very least, this data signifies that poor air quality does have an effect on the number of lung-related hospitalizations; its severity dependent on the kind of pollutant/s involved.


\section{Recommendations}
For future studies related to this research, it is recommended to utilize a larger dataset with shorter time intervals, e.g. daily data. The larger amount of data could possibly help the model detect more trends and increase the accuracy. 
In addition, based on the results of this research the FFDNN are warranted to be researched on due to the fact it performed better than the best RNN models. A study on FFDNN accuracy levels similar to this research is recommended.
For future proponents who will be conducting research related to hospitalization cases, it's recommended to use Davao Doctors Hospital as a source of data due to having a database system of their own and an extensive number of cases.
    \bibliographystyle{acm}
    \bibliography{ref}
\end{document}
